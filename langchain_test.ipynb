{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List, Annotated, Sequence, TypedDict\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.tools import BaseTool, Tool, tool\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "\n",
    "from langgraph.graph import END, Graph, StateGraph\n",
    "from langgraph.prebuilt import ToolExecutor, ToolInvocation\n",
    "from langgraph.graph import END, MessageGraph\n",
    "\n",
    "from langchain_core.agents import AgentAction, AgentFinish\n",
    "from langchain_core.messages import ToolMessage, HumanMessage, BaseMessage, FunctionMessage\n",
    "from langchain_core.tools import tool, BaseTool\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function, convert_to_openai_tool\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.tools import ShellTool\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "import json\n",
    "import subprocess\n",
    "import operator\n",
    "# import streamlit as st\n",
    "\n",
    "def create_agent(llm, tools, system_message):\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are a helpful AI assistant, helping the user accomplish their task.\"\n",
    "                \" Use the provided tools to progress towards answering the question.\"\n",
    "                \" You have access to the following tools: {tool_names}.\\n{system_message}\",\n",
    "            ),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        ]\n",
    "    )\n",
    "    prompt = prompt.partial(tool_names=\", \".join([tool.name for tool in tools]))\n",
    "    prompt = prompt.partial(system_message=system_message)\n",
    "\n",
    "    llm\n",
    "    return prompt | llm\n",
    "\n",
    "# @tool\n",
    "def multiply_calculator(\n",
    "    self, number1: Union[int, float], number2: Union[int, float]\n",
    ") -> Union[int, float]:\n",
    "    \"\"\"\n",
    "    This is the multiply calculator. It takes two numbers and returns their product.\n",
    "    \"\"\"\n",
    "    return number1 * number2\n",
    "\n",
    "class Multiply(BaseTool):\n",
    "    name = \"Multiply calculator\"\n",
    "    description = \"use this tool when you need to make a multiplication\"\n",
    "\n",
    "    def _run(\n",
    "        self, number1: Union[int, float], number2: Union[int, float]\n",
    "    ) -> Union[int, float]:\n",
    "        return number1 * number2\n",
    "\n",
    "    def _arun(self, number1: Union[int, float], number2: Union[int, float]):\n",
    "        raise NotImplementedError(\"This tool does not support async\")\n",
    "\n",
    "multiply_tools_dict = {\n",
    "    \"name\": \"multiply_calculator\",\n",
    "    \"description\": Multiply().description,\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"number1\": {\"type\": [\"integer\", \"number\"]},\n",
    "            \"number2\": {\"type\": [\"integer\", \"number\"]}\n",
    "        },\n",
    "        \"required\": [\"number1\", \"number2\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "multiply_function_call = {\"name\": \"multiply_calculator\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OllamaFunctions(model=\"llama3:8b\", format=\"json\")\n",
    "# model = model.bind_tools(\n",
    "#     tools=multiply_tools_dict,\n",
    "#     function_call=multiply_function_call\n",
    "# )\n",
    "# One experimental tool\n",
    "# model = model.bind_tools(\n",
    "#     tools=[\n",
    "#         {\n",
    "#             \"name\": \"multiply_calculator\",\n",
    "#             \"description\": \"use this tool when you need to make a multiplication\",\n",
    "#             \"parameters\": {\n",
    "#                 \"type\": \"object\",\n",
    "#                 \"properties\": {\n",
    "#                     \"number1\": {\n",
    "#                         \"type\": \"int\",\n",
    "#                         \"description\": \"First number\",\n",
    "#                     },\n",
    "#                     \"number2\": {\n",
    "#                         \"type\": \"int\",\n",
    "#                         \"description\": \"Second number\",\n",
    "#                     },\n",
    "#                 },\n",
    "#                 \"required\": [\"number1\", \"number2\"],\n",
    "#             },\n",
    "#         }\n",
    "#     ],\n",
    "#     function_call={\"name\": \"multiply_calculator\"},\n",
    "# )\n",
    "model.invoke(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "# model = Ollama(model=\"llama3:8b\")\n",
    "# Tools\n",
    "tools = [Multiply()]\n",
    "# Initialize the tool executor\n",
    "tool_executor = ToolExecutor(tools=tools)\n",
    "\n",
    "# Binding functions\n",
    "# functions = [convert_to_openai_function(t) for t in tools]\n",
    "# model = model.bind(tools=functions)\n",
    "\n",
    "# Create agent\n",
    "llm = create_agent(\n",
    "    llm=model,\n",
    "    tools=tools,\n",
    "    system_message=\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"messages\": [HumanMessage(content=\"Hi how are you doing?\")]}\n",
    "llm.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "\n",
    "# Define the function that determines whether to continue or not\n",
    "def should_continue(state):\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    # If there is no function call, then we finish\n",
    "    if \"function_call\" not in last_message.additional_kwargs:\n",
    "        return \"end\"\n",
    "    # Otherwise if there is, we continue\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state):\n",
    "    messages = state['messages']\n",
    "    response = model.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Define the function to execute tools\n",
    "def call_tool(state):\n",
    "    messages = state['messages']\n",
    "    # Based on the continue condition\n",
    "    # we know the last message involves a function call\n",
    "    last_message = messages[-1]\n",
    "    # We construct an ToolInvocation from the function_call\n",
    "    action = ToolInvocation(\n",
    "        tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
    "        tool_input=json.loads(last_message.additional_kwargs[\"function_call\"][\"arguments\"]),\n",
    "    )\n",
    "    # We call the tool_executor and get back a response\n",
    "    response = tool_executor.invoke(action)\n",
    "    # We use the response to create a FunctionMessage\n",
    "    function_message = FunctionMessage(content=str(response), name=action.tool)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [function_message]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize conversational memory\n",
    "# conversational_memory = ConversationBufferWindowMemory(\n",
    "#     memory_key=\"chat_history\", k=5, return_messages=True\n",
    "# )\n",
    "\n",
    "# initialize the graph\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "graph.add_node(\"agent\", call_model)\n",
    "graph.add_node(\"action\", call_tool)\n",
    "\n",
    "graph.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\": \"action\",\n",
    "        \"end\": END,\n",
    "    },\n",
    ")\n",
    "graph.add_edge(\"action\", \"agent\")\n",
    "\n",
    "graph.set_entry_point(\"agent\")\n",
    "\n",
    "runnable = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"messages\": [HumanMessage(content=\"Hi how are you doing?\")]}\n",
    "runnable.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# To get each step\n",
    "for output in runnable.stream(inputs):\n",
    "    # stream() yields dictionaries with output keyed by node name\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}':\")\n",
    "        print(\"---\")\n",
    "        print(value)\n",
    "    print(\"\\n---\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech-assistant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
